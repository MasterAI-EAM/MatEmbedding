{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc292a1f-7e69-4ac5-82a0-09315799977a",
   "metadata": {},
   "source": [
    "# finetuning for similarity tasks\n",
    "- STS Benchmark, widely used for semantic textual similarity, 8628 sentence pairs with similarity scores\n",
    "- CrisisTransformers datasets: MNR loss with GooAQ (question/answer), QQP (Quora Question pairs, anchor/positive/hard negative), AllNLI (SNLI+MultiNLI anchor/entailment/contradiction)\n",
    "- AllNLI and STSb datasets accessible with sentence-transformers, https://www.sbert.net/examples/datasets/README.html\n",
    "- GooAQ and QQP accessible with HuggingFace, https://huggingface.co/datasets/gooaq, https://huggingface.co/datasets/embedding-data/QQP_triplets\n",
    "- Natural Language Inference as the first fine-tuning step for sentence embedding methods, https://www.sbert.net/examples/training/nli/README.html\n",
    "- note the distribution of labels???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c5a616-3957-4910-8b1a-080668314943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging, torch\n",
    "import gzip, csv\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import models, losses, datasets, util\n",
    "from sentence_transformers import LoggingHandler, InputExample, SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "# for i in range(4):\n",
    "#     torch.cuda.set_device(i)\n",
    "#     torch.cuda.empty_cache()   # clear cache for each cuda\n",
    "    \n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()])\n",
    "\n",
    "## download datasets \n",
    "# util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', 'data/allnli')\n",
    "# util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', 'data/stsb')\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# qqp = load_dataset('embedding-data/QQP_triplets', split='train').shuffle(seed=100)   \n",
    "# iterable = iter(qqp)\n",
    "# first_item = next(iterable)\n",
    "# print(first_item)\n",
    "\n",
    "# with open('data/qqp.txt', 'w', encoding='utf8') as file_out: \n",
    "#     for item in iter(qqp):\n",
    "#         anchor = item['set']['query']\n",
    "#         pos = random.choice(item['set']['pos'])\n",
    "#         neg = random.choice(item['set']['neg'])\n",
    "#         file_out.write('{},{},{}\\n'.format(anchor, pos, neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90ffada-5916-4ba3-bdbb-99cf56246376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378992\n"
     ]
    }
   ],
   "source": [
    "## read data from allnli and qqp\n",
    "\n",
    "def add_to_samples(sent1, sent2, label):\n",
    "    if sent1 not in training_data.keys():\n",
    "        training_data[sent1] = {\"contradiction\":set(), \"entailment\":set(), \"neutral\":set()}\n",
    "    training_data[sent1][label].add(sent2)\n",
    "\n",
    "training_data = {}\n",
    "with gzip.open(\"../data/allnli\", \"rt\", encoding=\"utf8\") as file:\n",
    "    reader = csv.DictReader(file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"train\":\n",
    "            sent1 = row[\"sentence1\"].strip()\n",
    "            sent2 = row[\"sentence2\"].strip()\n",
    "            add_to_samples(sent1, sent2, row[\"label\"])\n",
    "\n",
    "training_samples = []\n",
    "for sent1,value in training_data.items():\n",
    "    if len(value['entailment']) > 0 and len(value['contradiction']) > 0:\n",
    "        training_samples.append(InputExample(texts = [sent1, random.choice(list(value['entailment'])), random.choice(list(value['contradiction']))]))\n",
    "\n",
    "with open('../data/qqp.txt', 'rt', encoding='utf8') as file_in:     \n",
    "    lines = file_in.readlines()\n",
    "    for line in lines:\n",
    "        sents = line.split(',')\n",
    "        training_samples.append(InputExample(texts=[sents[0], sents[1], sents[2]]))\n",
    "    \n",
    "print(len(training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5772a33-97e3-4060-9a49-2c620ac6b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 1379\n"
     ]
    }
   ],
   "source": [
    "## read stsb as development set and test set\n",
    "\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "with gzip.open('../data/stsb', \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        score = float(row[\"score\"]) / 5.0\n",
    "        if row[\"split\"] == \"dev\":\n",
    "            dev_samples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]], label=score))\n",
    "        elif row[\"split\"] == \"test\":\n",
    "            test_samples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]], label=score))\n",
    "\n",
    "print(len(dev_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9715d1-f7bd-49c9-bce3-c86a1c32fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matbert = models.Transformer('matbert-base-cased', max_seq_length=75)\n",
    "pooling = models.Pooling(matbert.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "model = SentenceTransformer(modules=[matbert, pooling], device='cuda:2')\n",
    "\n",
    "dataloader = datasets.NoDuplicatesDataLoader(training_samples, batch_size=256)   # usually larger batch size leads to better result\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, batch_size=256, name=\"sts-dev\")\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "num_epochs = 20\n",
    "warmup_steps = int(len(dataloader) * num_epochs * 0.1)\n",
    "output_path = 'outputs/matbert_mnr_triplet'\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "model.fit(\n",
    "    train_objectives = [(dataloader, loss)],\n",
    "    evaluator = dev_evaluator,   # evaluates the model after each epoch during training on development set to determine the best model to be saved\n",
    "    evaluation_steps = int(len(dataloader)),   # evaluated after 1480 steps, len(dataloader) is the number of batches\n",
    "    save_best_model = True,\n",
    "    epochs = num_epochs,\n",
    "    warmup_steps = warmup_steps,\n",
    "    output_path = output_path,\n",
    "    use_amp = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38b077-9411-40d1-b133-f28bcf91cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation\n",
    "\n",
    "tuned_model = SentenceTransformer(output_path)\n",
    "\n",
    "# stsb test samples\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, batch_size=512, name=\"sts-test\")\n",
    "test_evaluator(tuned_model, output_path=output_path)   # results written in csv\n",
    "\n",
    "# spearman\n",
    "with open('data/zt_ori_84.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    names = [line.strip().split('\\t')[0] for line in lines]\n",
    "    zt_scores = [line.strip().split('\\t')[-1] for line in lines]\n",
    "    \n",
    "center_embedding = tuned_model.encode('thermoelectric')\n",
    "name_embeddings = tuned_model.encode(names)\n",
    "cos_sims = [(1-cosine(center_embedding,name_embedding)) for name_embedding in name_embeddings]\n",
    "corr, pvalue = stats.spearmanr(cos_sims, zt_scores)\n",
    "print('spearman correlation', corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5aa9da-6baf-4377-aeb9-d9158742d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped after epoch 10, best result (stsb dev 0.85) at epoch 5 saved to matbert_mnr_triplet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
